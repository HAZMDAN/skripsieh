{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Flatten\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.fft import fft\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "url = 'https://archive.physionet.org/physiobank/database/challenge/2015/training/RECORDS'\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "\n",
        "    text = response.text\n",
        "    print(\"Successfully retrieve the patient Id's Status code:\", response.status_code)\n",
        "else:\n",
        "    print(\"Failed to retrieve the text content. Status code:\", response.status_code)\n",
        "\n",
        "lines = text.strip().split('\\n')\n",
        "\n",
        "csv_file = 'record_names.csv'\n",
        "\n",
        "with open(csv_file, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows([[line] for line in lines])\n",
        "\n",
        "print(f'CSV file \"{csv_file}\" has been created.')\n",
        "\n",
        "\n",
        "csv_file = 'record_names.csv'\n",
        "\n",
        "patient_ids_df = pd.read_csv(csv_file, header=None)\n",
        "\n",
        "patient_ids = patient_ids_df.to_numpy().reshape(-1)\n",
        "\n",
        "print(patient_ids)"
      ],
      "metadata": {
        "id": "RcVEi2qp2atq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "RcVEi2qp2atq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction - Add the most dense frequency capture\n",
        "def capture_dominant_frequency(signal, fs):\n",
        "    # Perform Fourier Transform on the signal to get frequency components\n",
        "    N = len(signal)\n",
        "    freqs = np.fft.fftfreq(N, d=1/fs)\n",
        "    fft_vals = fft(signal)\n",
        "\n",
        "    # Get the magnitude of the FFT results\n",
        "    magnitude = np.abs(fft_vals)\n",
        "\n",
        "    # Find the index of the highest magnitude, which corresponds to the dominant frequency\n",
        "    dominant_frequency_index = np.argmax(magnitude[1:]) + 1  # Exclude the DC component\n",
        "    dominant_frequency = np.abs(freqs[dominant_frequency_index])\n",
        "\n",
        "    return dominant_frequency\n",
        "\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "def extract_ppg_features(ppg_signal, fs):\n",
        "    # Peak Detection with higher threshold\n",
        "    peak_height_threshold = 0.5 * (np.percentile(ppg_signal, 75) + np.median(ppg_signal)) # Might have to adjust accordingly\n",
        "    peaks, _ = find_peaks(ppg_signal, height=peak_height_threshold)\n",
        "\n",
        "    peak_features = []\n",
        "\n",
        "    prev_peak_index = None\n",
        "\n",
        "    for peak_index in peaks:\n",
        "\n",
        "        window_size = int(fs * 2)  # Example: 0.5 seconds window\n",
        "        peak_window = ppg_signal[max(0, peak_index - window_size):min(len(ppg_signal), peak_index + window_size)]\n",
        "\n",
        "        # Morphological Features\n",
        "        sa = np.max(peak_window)\n",
        "        Da = np.min(peak_window)\n",
        "        SA = np.trapz(peak_window, dx=1/fs)\n",
        "        DA = np.trapz(peak_window, dx=1/fs) - SA\n",
        "        St = peak_index / fs\n",
        "        Dt = (len(ppg_signal) - peak_index) / fs\n",
        "\n",
        "        # Frequency Domain Features\n",
        "        fft_result = fft(peak_window)\n",
        "        magnitude_spectrum = np.abs(fft_result)\n",
        "        frequency_spectrum = np.fft.fftfreq(len(peak_window), d=1/fs)\n",
        "        dominant_frequency = frequency_spectrum[np.argmax(magnitude_spectrum)]\n",
        "        pmf = magnitude_spectrum / np.sum(magnitude_spectrum)\n",
        "        spectral_entropy = -np.sum(pmf * np.log2(pmf + 1e-10))\n",
        "\n",
        "        # Additional features\n",
        "        if prev_peak_index is not None:\n",
        "            ppi = (peak_index - prev_peak_index) / fs\n",
        "        else:\n",
        "            ppi = 0\n",
        "\n",
        "        pi = ppi\n",
        "        pw = calculate_pulse_width(peak_window, fs)\n",
        "        fwhm = calculate_fwhm(peak_window, fs)\n",
        "\n",
        "        signal_area = np.trapz(ppg_signal, dx=1/fs)\n",
        "        rise_time = St - (max(0, peak_index - window_size)) / fs\n",
        "        fall_time = (min(len(ppg_signal), peak_index + window_size) - peak_index) / fs\n",
        "\n",
        "        amplitude_modulation_depth = sa - Da\n",
        "        energy = np.sum(peak_window ** 2)\n",
        "        zero_crossing_rate = calculate_zero_crossing_rate(peak_window)\n",
        "\n",
        "        # Statistical Features\n",
        "        mean = np.mean(peak_window)\n",
        "        median = np.median(peak_window)\n",
        "        std_deviation = np.std(peak_window)\n",
        "        skewness = stats.skew(peak_window)\n",
        "        kurtosis = stats.kurtosis(peak_window)\n",
        "        min_value = np.min(peak_window)\n",
        "        max_value = np.max(peak_window)\n",
        "        variance = np.var(peak_window)\n",
        "\n",
        "        # Additional features\n",
        "        slope = calculate_slope(peak_window, fs)\n",
        "        peak_count = len(peaks)\n",
        "\n",
        "        # Handle exceptions for divide by zero errors\n",
        "        try:\n",
        "            amplitude_ratio = sa / Da\n",
        "            area_ratio = SA / DA\n",
        "            interval_ratio = pi / ppi if ppi != 0 else 0\n",
        "        except ZeroDivisionError:\n",
        "            amplitude_ratio = 0\n",
        "            area_ratio = 0\n",
        "            interval_ratio = 0\n",
        "\n",
        "        # Append all features to the list\n",
        "        peak_features.append({\n",
        "            'sa': sa, 'Da': Da, 'SA': SA, 'DA': DA, 'St': St, 'Dt': Dt,\n",
        "            'PI': pi, 'PPI': ppi, 'PW': pw, 'FWHM': fwhm,\n",
        "            'Dominant_frequency': dominant_frequency,\n",
        "            'Spectral_entropy': spectral_entropy,\n",
        "            'Signal_area': signal_area,\n",
        "            'Rise_time': rise_time,\n",
        "            'Fall_time': fall_time,\n",
        "            'Amplitude_modulation_depth': amplitude_modulation_depth,\n",
        "            'Energy': energy,\n",
        "            'Zero_crossing_rate': zero_crossing_rate,\n",
        "            'Mean': mean,\n",
        "            'Median': median,\n",
        "            'Standard_deviation': std_deviation,\n",
        "            'Skewness': skewness,\n",
        "            'Kurtosis': kurtosis,\n",
        "            'Min_value': min_value,\n",
        "            'Max_value': max_value,\n",
        "            'Variance': variance,\n",
        "            'Slope': slope,\n",
        "            'Peak_count': peak_count,\n",
        "            'Amplitude_ratio': amplitude_ratio,\n",
        "            'Area_ratio': area_ratio,\n",
        "            'Interval_ratio': interval_ratio\n",
        "            # Add more features as needed\n",
        "        })\n",
        "\n",
        "        prev_peak_index = peak_index\n",
        "\n",
        "    return peak_features, peaks\n",
        "\n",
        "def calculate_slope(signal, fs):\n",
        "    time = np.arange(0, len(signal)) / fs\n",
        "    slope, _ = np.polyfit(time, signal, 1)\n",
        "    return slope\n",
        "\n",
        "def calculate_pulse_width(peak_window, fs):\n",
        "    half_max = 0.5 * np.max(peak_window)\n",
        "    above_half = peak_window >= half_max\n",
        "    crossings = np.where(above_half)[0]\n",
        "    if len(crossings) < 2:\n",
        "        return 0.0\n",
        "    return (crossings[-1] - crossings[0]) / fs\n",
        "\n",
        "def calculate_fwhm(peak_window, fs):\n",
        "    half_max = 0.5 * np.max(peak_window)\n",
        "    cross_indices = np.where(peak_window >= half_max)[0]\n",
        "    if len(cross_indices) == 0:\n",
        "        return 0.0\n",
        "    return (cross_indices[-1] - cross_indices[0]) / fs\n",
        "\n",
        "def calculate_zero_crossing_rate(signal):\n",
        "    # Calculate zero crossing rate\n",
        "    return np.sum(np.diff(np.sign(signal)) != 0) / len(signal)\n"
      ],
      "metadata": {
        "id": "h6p6j5tb3M7v"
      },
      "id": "h6p6j5tb3M7v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PCA to reduce dimensionality of extracted features\n",
        "def apply_pca(features, n_components=10):\n",
        "    pca = PCA(n_components=n_components)\n",
        "    scaled_features = StandardScaler().fit_transform(features)  # Scale features before PCA\n",
        "    pca_features = pca.fit_transform(scaled_features)\n",
        "    return pca_features\n",
        "\n",
        "# Prepare dataset with PCA\n",
        "dataset = []\n",
        "i = 0\n",
        "for label, patient_ids in patient_ids_by_activity.items():\n",
        "    for patient_id in patient_ids:\n",
        "        if patient_id not in pleth_signals:\n",
        "            print(f\"Patient ID {patient_id} not found in pleth_signal dictionary\")\n",
        "            continue\n",
        "        fs = pleth_infos[patient_id]['frequency']\n",
        "        segmented_signal = segment_signal(pleth_signals[patient_id], 10, fs)\n",
        "\n",
        "        for segment in segmented_signal:\n",
        "            segment_processed = bandpass_filter(segment, lowcut=0.05, highcut=10, fs=fs)\n",
        "            segment_processed = moving_average(segment_processed, window_size=3)\n",
        "            segment_processed = remove_baseline_wandering(segment_processed)\n",
        "\n",
        "            if not any(segment_processed):\n",
        "                continue\n",
        "\n",
        "            features = extract_ppg_features(segment_processed, fs)\n",
        "            for feature in features:\n",
        "                peak_array = [feature[key] for key in feature.keys()]\n",
        "                dataset.append((label, peak_array))\n",
        "                i += 1\n",
        "\n",
        "# Prepare labels and features for training\n",
        "labels = [label for label, _ in dataset]\n",
        "signals = [signal for _, signal in dataset]\n",
        "\n",
        "# One-hot encode the labels\n",
        "unique_labels = set(labels)\n",
        "label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
        "index_to_label = {i: label for label, i in label_to_index.items()}\n",
        "encoded_labels = [to_categorical(label_to_index[label], num_classes=len(unique_labels)) for label in labels]\n",
        "\n",
        "# Apply PCA to features\n",
        "features = [feature for _, feature in dataset]\n",
        "pca_features = apply_pca(features, n_components=10)\n",
        "\n",
        "# Padding the sequences\n",
        "max_length = max(len(signal) for signal in signals)\n",
        "padded_signals = [np.pad(signal, (0, max_length - len(signal))) for signal in signals]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(padded_signals)\n",
        "y = np.array(encoded_labels)\n",
        "\n",
        "# Add PCA features as additional input\n",
        "X_pca = np.hstack([X.reshape(X.shape[0], -1), pca_features])\n",
        "\n",
        "# Reshape for LSTM/BiLSTM input (no PCA in this section)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshaping for LSTM/BiLSTM input\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n"
      ],
      "metadata": {
        "id": "shZbkwNE3Z7C"
      },
      "id": "shZbkwNE3Z7C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create LSTM model with ANN after LSTM\n",
        "def create_lstm_ann_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        LSTM(64, return_sequences=True, input_shape=input_shape),\n",
        "        LSTM(64),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(32, activation='relu'),  # Adding ANN layer after LSTM\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create BiLSTM model with ANN after BiLSTM\n",
        "def create_bilstm_ann_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Bidirectional(LSTM(64, return_sequences=True), input_shape=input_shape),\n",
        "        Bidirectional(LSTM(64)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(32, activation='relu'),  # Adding ANN layer after BiLSTM\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Model Creation\n",
        "input_shape = X_train.shape[1:]  # Shape of input features\n",
        "num_classes = len(unique_labels)\n",
        "\n",
        "lstm_ann_model = create_lstm_ann_model(input_shape, num_classes)\n",
        "bilstm_ann_model = create_bilstm_ann_model(input_shape, num_classes)\n",
        "\n",
        "# Compile the models\n",
        "lstm_ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "bilstm_ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the models\n",
        "history_lstm_ann = lstm_ann_model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n",
        "history_bilstm_ann = bilstm_ann_model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "# Model evaluation\n",
        "lstm_evaluation = lstm_ann_model.evaluate(X_test, y_test)\n",
        "bilstm_evaluation = bilstm_ann_model.evaluate(X_test, y_test)\n",
        "print(\"LSTM+ANN Evaluation:\", lstm_evaluation)\n",
        "print(\"BiLSTM+ANN Evaluation:\", bilstm_evaluation)\n",
        "\n",
        "# Save models\n",
        "lstm_ann_model.save('/home/nakul/Documents/Python/Arrhythmia_Classification_PPG/lstm_ann_model.h5')\n",
        "bilstm_ann_model.save('/home/nakul/Documents/Python/Arrhythmia_Classification_PPG/bilstm_ann_model.h5')\n",
        "\n",
        "# Plotting the training and testing accuracy/loss\n",
        "epochs = [i for i in range(50)]\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "\n",
        "# LSTM+ANN Accuracy and Loss\n",
        "ax[0].plot(epochs, history_lstm_ann.history['loss'], label='Training Loss')\n",
        "ax[0].plot(epochs, history_lstm_ann.history['val_loss'], label='Testing Loss')\n",
        "ax[0].set_title('LSTM+ANN Loss')\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "ax[0].set_ylim(0, 1)\n",
        "\n",
        "ax[1].plot(epochs, history_lstm_ann.history['accuracy'], label='Training Accuracy')\n",
        "ax[1].plot(epochs, history_lstm_ann.history['val_accuracy'], label='Testing Accuracy')\n",
        "ax[1].set_title('LSTM+ANN Accuracy')\n",
        "ax[1].legend()\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylim(0, 1)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qb_N2hPQ3iVH"
      },
      "id": "qb_N2hPQ3iVH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# BiLSTM+ANN Accuracy and Loss\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "\n",
        "ax[0].plot(epochs, history_bilstm_ann.history['loss'], label='Training Loss')\n",
        "ax[0].plot(epochs, history_bilstm_ann.history['val_loss'], label='Testing Loss')\n",
        "ax[0].set_title('BiLSTM+ANN Loss')\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "ax[0].set_ylim(0, 1)\n",
        "\n",
        "ax[1].plot(epochs, history_bilstm_ann.history['accuracy'], label='Training Accuracy')\n",
        "ax[1].plot(epochs, history_bilstm_ann.history['val_accuracy'], label='Testing Accuracy')\n",
        "ax[1].set_title('BiLSTM+ANN Accuracy')\n",
        "ax[1].legend()\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylim(0, 1)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix and Final Evaluation\n",
        "lstm_predictions = lstm_ann_model.predict(X_test)\n",
        "bilstm_predictions = bilstm_ann_model.predict(X_test)\n",
        "\n",
        "lstm_pred_labels = np.argmax(lstm_predictions, axis=1)\n",
        "bilstm_pred_labels = np.argmax(bilstm_predictions, axis=1)\n",
        "\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Confusion Matrix for LSTM+ANN and BiLSTM+ANN\n",
        "conf_matrix_lstm_ann = confusion_matrix(y_test_labels, lstm_pred_labels)\n",
        "conf_matrix_bilstm_ann = confusion_matrix(y_test_labels, bilstm_pred_labels)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "sns.heatmap(conf_matrix_lstm_ann, annot=True, fmt='d', cmap='Blues', ax=ax[0], xticklabels=class_labels, yticklabels=class_labels)\n",
        "ax[0].set_title('Confusion Matrix - LSTM+ANN')\n",
        "\n",
        "sns.heatmap(conf_matrix_bilstm_ann, annot=True, fmt='d', cmap='Blues', ax=ax[1], xticklabels=class_labels, yticklabels=class_labels)\n",
        "ax[1].set_title('Confusion Matrix - BiLSTM+ANN')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b64zlmcZ3mlA"
      },
      "id": "b64zlmcZ3mlA",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}